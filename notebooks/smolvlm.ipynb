{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mẫu trong CSV: 8278\n",
      "0    [1, 7, 8, 10, 11]\n",
      "1               [8, 9]\n",
      "2                 [10]\n",
      "3             [10, 11]\n",
      "4               [8, 9]\n",
      "Name: label, dtype: object\n",
      "labels shape: (8278, 18)\n",
      "label classes: ['0' '1' '10' '11' '12' '13' '14' '15' '16' '17' '2' '3' '4' '5' '6' '7'\n",
      " '8' '9']\n",
      "train_labels shape: (6622, 18)\n",
      "val_labels shape: (1656, 18)\n",
      "len(train_audio_paths): 6622\n",
      "len(val_audio_paths): 1656\n",
      "train_labels dtype: int64\n",
      "Sample train_labels[0]: [0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_labels dtype after conversion: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([18, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import (\n",
    "    Wav2Vec2Model, \n",
    "    Wav2Vec2Processor, \n",
    "    AutoFeatureExtractor, \n",
    "    ASTForAudioClassification\n",
    ")\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelAveragePrecision\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Đọc và chuẩn hóa dữ liệu ---\n",
    "data = pd.read_csv(\"audio_only_classification_dataset.csv\")\n",
    "print(f\"Số mẫu trong CSV: {len(data)}\")\n",
    "data['label'] = data['label'].apply(lambda x: [label.strip() for label in x.split(',')])  # Chuẩn hóa nhãn\n",
    "print(data['label'].head())\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(data['label'])\n",
    "label_classes = mlb.classes_\n",
    "print(f\"labels shape: {labels.shape}\")\n",
    "print(f\"label classes: {label_classes}\")\n",
    "\n",
    "audio_paths = data['audio_id'].values\n",
    "train_audio_paths, val_audio_paths, train_labels, val_labels = train_test_split(\n",
    "    audio_paths, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"train_labels shape: {train_labels.shape}\")\n",
    "print(f\"val_labels shape: {val_labels.shape}\")\n",
    "print(f\"len(train_audio_paths): {len(train_audio_paths)}\")\n",
    "print(f\"len(val_audio_paths): {len(val_audio_paths)}\")\n",
    "print(f\"train_labels dtype: {train_labels.dtype}\")\n",
    "print(f\"Sample train_labels[0]: {train_labels[0]}\")\n",
    "\n",
    "# Chuyển đổi nhãn sang float32\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "val_labels = val_labels.astype(np.float32)\n",
    "print(f\"train_labels dtype after conversion: {train_labels.dtype}\")\n",
    "\n",
    "# --- Tiền xử lý âm thanh ---\n",
    "def preprocess_audio(audio_path, sample_rate= 16000):\n",
    "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    return y, sr\n",
    "\n",
    "# --- Transform cho Wav2Vec2 ---\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", return_attention_mask=False)\n",
    "def transform_waveform(audio_path):\n",
    "    y, sr = preprocess_audio(audio_path)\n",
    "    features = feature_extractor(y, sampling_rate=sr, return_tensors=\"pt\", padding=\"max_length\")\n",
    "    input_values = features['input_values'].squeeze(0)  # [time_frames, frequency_bins] hoặc [1, time_frames, frequency_bins]\n",
    "    return input_values\n",
    "\n",
    "# --- Định nghĩa Dataset ---\n",
    "class AudioMultilabelDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, sample_rate= 16000, max_length=5.0):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = max_length\n",
    "        # print(f\"Dataset labels shape: {labels.shape}\")\n",
    "        assert len(audio_paths) == labels.shape[0], f\"Số mẫu không khớp: {len(audio_paths)} vs {labels.shape[0]}\"\n",
    "        assert labels.shape[1] == len(label_classes), f\"Số nhãn không khớp: {labels.shape[1]} vs {len(label_classes)}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        waveform = transform_waveform(audio_path)\n",
    "        \n",
    "        labels_tensor = torch.FloatTensor(label).to(dtype=torch.float32)\n",
    "        # print(f\"Label dtype: {labels_tensor.dtype}\")\n",
    "        return {\n",
    "            'input_values': waveform,\n",
    "            'labels': labels_tensor\n",
    "        }\n",
    "\n",
    "# --- Tạo dataset ---\n",
    "train_dataset = AudioMultilabelDataset(train_audio_paths, train_labels)\n",
    "val_dataset = AudioMultilabelDataset(val_audio_paths, val_labels)\n",
    "\n",
    "# --- Collate function ---\n",
    "def collate_fn(batch):\n",
    "    input_values = torch.stack([item['input_values'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\n",
    "        'input_values': input_values,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# --- Tạo DataLoader ---\n",
    "batch_size = 8  # Có thể thử batch_size=1 để debug\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- Khởi tạo mô hình, hàm mất mát, optimizer ---\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ASTForAudioClassification.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593', \n",
    "                            num_labels=len(label_classes),\n",
    "                            problem_type=\"multi_label_classification\",\n",
    "                            ignore_mismatched_sizes=True).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Phù hợp cho phân loại đa nhãn\n",
    "\n",
    "# Thiết lập optimizer với learning rate thích hợp và weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "\n",
    "# Thêm learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "# Gradient accumulation cho batch size lớn hơn trên GPU nhỏ\n",
    "accumulation_steps = 2\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.amp.GradScaler(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits, labels, device):\n",
    "    # Sử dụng torch.sigmoid để có giá trị dự đoán tốt hơn\n",
    "    probs = torch.sigmoid(logits)\n",
    "    binary_preds = (probs > 0.5).int()\n",
    "    \n",
    "    # Chuyển nhãn sang int32 cho torchmetrics\n",
    "    labels = labels.to(dtype=torch.int32)\n",
    "    \n",
    "    # Tính toán metrics một lần trên batch\n",
    "    acc = MultilabelAccuracy(num_labels=labels.shape[1], average='macro').to(device)\n",
    "    mini_acc= MultilabelAccuracy(num_labels=labels.shape[1], average='macro').to(device)\n",
    "    precision_metric = MultilabelPrecision(num_labels=labels.shape[1], average='macro').to(device)\n",
    "    mAP = MultilabelAveragePrecision(num_labels=labels.shape[1]).to(device)\n",
    "    \n",
    "    acc_value = acc(binary_preds, labels).item()\n",
    "    min_acc_value = mini_acc(binary_preds, labels).item()\n",
    "    precision_value = precision_metric(binary_preds, labels).item()\n",
    "    map_value = mAP(probs, labels).item()  # Sử dụng probs thay vì logits\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc_value,\n",
    "        'mini_accuracy': min_acc_value,\n",
    "        'precision': precision_value,\n",
    "        'mAP': map_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 828/828 [29:45<00:00,  2.16s/it]\n",
      "Validation: 100%|██████████| 207/207 [06:12<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model!\n",
      "Epoch 1/3, Time: 2157.79s, LR: 0.000200\n",
      "Train Loss: 0.3054, Train Accuracy: 0.8804,  Train Mini Accuracy: 0.8804,  Train Precision: 0.0266, Train mAP: 0.2198\n",
      "Val Loss: 0.3055, Val Accuracy: 0.8769,  Val Mini Accuracy: 0.8769, Val Precision: 0.0300, Val mAP: 0.1379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:   3%|▎         | 22/828 [00:46<28:26,  2.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     43\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     46\u001b[0m     input_values \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     47\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 78\u001b[0m, in \u001b[0;36mAudioMultilabelDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     76\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_paths[idx]\n\u001b[0;32m     77\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m---> 78\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_waveform\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m labels_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(label)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# print(f\"Label dtype: {labels_tensor.dtype}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 56\u001b[0m, in \u001b[0;36mtransform_waveform\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform_waveform\u001b[39m(audio_path):\n\u001b[1;32m---> 56\u001b[0m     y, sr \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     features \u001b[38;5;241m=\u001b[39m feature_extractor(y, sampling_rate\u001b[38;5;241m=\u001b[39msr, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m     input_values \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [time_frames, frequency_bins] hoặc [1, time_frames, frequency_bins]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m, in \u001b[0;36mpreprocess_audio\u001b[1;34m(audio_path, sample_rate)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_audio\u001b[39m(audio_path, sample_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16000\u001b[39m):\n\u001b[1;32m---> 50\u001b[0m     y, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y, sr\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m         y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\librosa\\core\\audio.py:222\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    219\u001b[0m         frame_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43msf_desc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, sr_native\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\soundfile.py:942\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(out):\n\u001b[0;32m    941\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[1;32m--> 942\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m>\u001b[39m frames:\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\soundfile.py:1394\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[1;34m(self, action, array, frames)\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39msizeof(ctype)\n\u001b[0;32m   1393\u001b[0m cdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mcast(ctype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, array\u001b[38;5;241m.\u001b[39m__array_interface__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\soundfile.py:1403\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[1;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[0;32m   1401\u001b[0m     curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m   1402\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m action \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ctype)\n\u001b[1;32m-> 1403\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1404\u001b[0m _error_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errorcode)\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compute_metrics(logits, labels, device):\n",
    "    # Sử dụng torch.sigmoid để có giá trị dự đoán tốt hơn\n",
    "    probs = torch.sigmoid(logits)\n",
    "    binary_preds = (probs > 0.5).int()\n",
    "    \n",
    "    # Chuyển nhãn sang int32 cho torchmetrics\n",
    "    labels = labels.to(dtype=torch.int32)\n",
    "    \n",
    "    # Tính toán metrics một lần trên batch\n",
    "    acc = MultilabelAccuracy(num_labels=labels.shape[1], average='macro').to(device)\n",
    "    mini_acc= MultilabelAccuracy(num_labels=labels.shape[1], average='macro').to(device)\n",
    "    precision_metric = MultilabelPrecision(num_labels=labels.shape[1], average='macro').to(device)\n",
    "    mAP = MultilabelAveragePrecision(num_labels=labels.shape[1]).to(device)\n",
    "    \n",
    "    acc_value = acc(binary_preds, labels).item()\n",
    "    min_acc_value = mini_acc(binary_preds, labels).item()\n",
    "    precision_value = precision_metric(binary_preds, labels).item()\n",
    "    map_value = mAP(probs, labels).item()  # Sử dụng probs thay vì logits\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc_value,\n",
    "        'mini_accuracy': min_acc_value,\n",
    "        'precision': precision_value,\n",
    "        'mAP': map_value\n",
    "    }\n",
    "# Early stopping\n",
    "best_val_map = 0\n",
    "patience = 1\n",
    "patience_counter = 0\n",
    "early_stop = False\n",
    "\n",
    "num_epochs = 3  # Tăng số epochs\n",
    "for epoch in range(num_epochs):\n",
    "    if early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "        \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_metrics = {'accuracy': 0.0, 'mini_accuracy': 0.0, 'precision': 0.0, 'mAP': 0.0}\n",
    "    num_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Sử dụng mixed precision\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(input_values)  # outputs là SequenceClassifierOutput\n",
    "            logits = outputs.logits  # Lấy tensor logits [batch_size, num_labels]\n",
    "            loss = criterion(logits, labels) / accumulation_steps\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            # Gradient clipping để tránh exploding gradients\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        # Tính metrics chỉ với một tỷ lệ nhất định các batch để tăng tốc\n",
    "        if i % 5 == 0:  # Tính metrics mỗi 5 batch\n",
    "            metrics = compute_metrics(logits, labels, device)\n",
    "            for key in train_metrics:\n",
    "                train_metrics[key] += metrics[key]\n",
    "            num_batches += 1\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    for key in train_metrics:\n",
    "        train_metrics[key] /= max(num_batches, 1)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_metrics = {'accuracy': 0.0, 'mini_accuracy': 0.0, 'precision': 0.0, 'mAP': 0.0}\n",
    "    num_batches = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(input_values)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Thu thập tất cả logits và labels để tính metrics một lần duy nhất\n",
    "            all_logits.append(logits.detach())\n",
    "            all_labels.append(labels.detach())\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Tính metrics một lần trên toàn bộ validation set\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    val_metrics = compute_metrics(all_logits, all_labels, device)\n",
    "    \n",
    "    val_loss /= num_batches\n",
    "    \n",
    "    # Cập nhật learning rate dựa trên validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_metrics['mAP'] > best_val_map:\n",
    "        best_val_map = val_metrics['mAP']\n",
    "        patience_counter = 0\n",
    "        # Lưu model tốt nhất\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"Saved best model!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            early_stop = True\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Time: {epoch_time:.2f}s, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics['accuracy']:.4f},  Train Mini Accuracy: {train_metrics['mini_accuracy']:.4f}, \", \n",
    "          f\"Train Precision: {train_metrics['precision']:.4f}, Train mAP: {train_metrics['mAP']:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics['accuracy']:.4f},  Val Mini Accuracy: {val_metrics['mini_accuracy']:.4f}, \"\n",
    "          f\"Val Precision: {val_metrics['precision']:.4f}, Val mAP: {val_metrics['mAP']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTForAudioClassification(\n",
       "  (audio_spectrogram_transformer): ASTModel(\n",
       "    (embeddings): ASTEmbeddings(\n",
       "      (patch_embeddings): ASTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ASTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ASTLayer(\n",
       "          (attention): ASTSdpaAttention(\n",
       "            (attention): ASTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): ASTMLPHead(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
